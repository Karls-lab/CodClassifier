{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a91feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T01:04:55.157807Z",
     "iopub.status.busy": "2023-10-12T01:04:55.156529Z",
     "iopub.status.idle": "2023-10-12T01:05:02.250755Z",
     "shell.execute_reply": "2023-10-12T01:05:02.250084Z",
     "shell.execute_reply.started": "2023-10-12T01:03:19.508228Z"
    },
    "papermill": {
     "duration": 7.111828,
     "end_time": "2023-10-12T01:05:02.250928",
     "exception": false,
     "start_time": "2023-10-12T01:04:55.139100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4445437f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T01:05:21.156255Z",
     "iopub.status.busy": "2023-10-12T01:05:21.154804Z",
     "iopub.status.idle": "2023-10-12T01:05:21.158952Z",
     "shell.execute_reply": "2023-10-12T01:05:21.159748Z",
     "shell.execute_reply.started": "2023-10-12T01:03:27.362389Z"
    },
    "papermill": {
     "duration": 0.100768,
     "end_time": "2023-10-12T01:05:21.159960",
     "exception": false,
     "start_time": "2023-10-12T01:05:21.059192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.zip', 'sample_submission_stg1.csv', '__MACOSX', 'valid', 'sample_submission_stg1.csv.zip', 'test_stg2', 'sample_submission_stg2.csv.zip', 'test_stg2.7z', 'test_stg1.zip', 'train', 'test_stg1']\n",
      "Validation folder already contains files for ALB\n",
      "Validation folder already contains files for BET\n",
      "Validation folder already contains files for DOL\n",
      "Validation folder already contains files for LAG\n",
      "Validation folder already contains files for NoF\n",
      "Validation folder already contains files for OTHER\n",
      "Validation folder already contains files for SHARK\n",
      "Validation folder already contains files for YFT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset should be unzipped into the folder named 'data' This folder should be in the same directory as this file.\n",
    "Extract the train.zip file.\n",
    "\"\"\"\n",
    "print(os.listdir(\"data/\")) # should list train.zip, sample_submission... etc \n",
    "# The Most important folder is going to be the train folder. \n",
    "# We will create our validation set by splitting the train folder into train and valid folders.\n",
    "if not os.path.exists('data/valid'):\n",
    "    os.mkdir('data/valid/')\n",
    "\n",
    "# Now we will create validation folders for each type of fish \n",
    "fish_types = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'] \n",
    "for fish in fish_types:\n",
    "    # Creates a validation folder for each type of feature (fish)\n",
    "    if not os.path.exists(f'data/valid/{fish}'):\n",
    "        os.mkdir(f'data/valid/{fish}')\n",
    "\n",
    "    # if valid folder already contains validation set, it breaks out of the loop\n",
    "    if len(os.listdir(f'data/valid/{fish}')) > 0:\n",
    "        print(f\"Validation folder already contains files for {fish}\")\n",
    "        continue\n",
    "\n",
    "    train_dir = f'data/train/{fish}/'\n",
    "    valid_dir = f'data/valid/{fish}/'\n",
    "\n",
    "    # List all files in the source directory\n",
    "    file_paths = [os.path.join(train_dir, filename) for filename in os.listdir(train_dir)]\n",
    "\n",
    "    # Use 20% of the files for validation, using the train_test_split function\n",
    "    validation_ratio = 0.2\n",
    "    train_files, valid_files = train_test_split(file_paths, test_size=validation_ratio, random_state=42)\n",
    "\n",
    "    # Move the selected validation files to the validation directory\n",
    "    for file_path in valid_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        destination = os.path.join(valid_dir, filename)\n",
    "        os.rename(file_path, destination)\n",
    "\n",
    "    print(f\"{len(valid_files)} files from test/{fish} moved to validation set.\")\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Create a function to reset the validation, moving all validation images back into the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d877c8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T01:05:23.125102Z",
     "iopub.status.busy": "2023-10-12T01:05:23.124327Z",
     "iopub.status.idle": "2023-10-12T01:05:23.136800Z",
     "shell.execute_reply": "2023-10-12T01:05:23.137567Z"
    },
    "papermill": {
     "duration": 0.12079,
     "end_time": "2023-10-12T01:05:23.137832",
     "exception": false,
     "start_time": "2023-10-12T01:05:23.017042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "695f883c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T01:05:23.511770Z",
     "iopub.status.busy": "2023-10-12T01:05:23.511087Z",
     "iopub.status.idle": "2023-10-12T01:05:23.830498Z",
     "shell.execute_reply": "2023-10-12T01:05:23.829851Z"
    },
    "papermill": {
     "duration": 0.41012,
     "end_time": "2023-10-12T01:05:23.830677",
     "exception": false,
     "start_time": "2023-10-12T01:05:23.420557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3019 images belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 758 images belonging to 8 classes.\n",
      "{'ALB': 0, 'BET': 1, 'DOL': 2, 'LAG': 3, 'NoF': 4, 'OTHER': 5, 'SHARK': 6, 'YFT': 7}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "This part of the code sets up an Image generator that will be used for the CNN\n",
    "It doesn't actually compute any of the resizing or transformations yet, it just sets up the generator\n",
    "The generator will be called when we train the model \n",
    "\"\"\"\n",
    "# Create an ImageDataGenerator with data augmentation and normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,    # Normalize pixel values to the range [0, 1]\n",
    "    shear_range=0.2,    # Random shear transformations\n",
    "    zoom_range=0.2,     # Random zooming\n",
    "    horizontal_flip=True,  # Random horizontal flipping\n",
    ")\n",
    "\n",
    "# Load and preprocess the dataset (in this example, assume you have a directory of images)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'data/train/',\n",
    "    target_size=(224, 224),   # Resize images to a consistent size\n",
    "    batch_size=32,            # Batch size for training\n",
    "    class_mode='categorical'  # The type of labels (categorical for classification)\n",
    ")\n",
    "\n",
    "# Load and preprocess the validation set\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'data/valid/',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4391f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 394272)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               50466944  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50468872 (192.52 MB)\n",
      "Trainable params: 50468872 (192.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "10/10 [==============================] - 11s 1s/step - loss: 21.0714 - accuracy: 0.2313 - val_loss: 5.4535 - val_accuracy: 0.3750\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part of the code sets up the CNN model\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Run the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=10,\n",
    "    epochs=1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "version = 1\n",
    "model.save_weights(f'model{version}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22cce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_stg1/img_06658.jpg', 'test_stg1/img_06525.jpg', 'test_stg1/img_04860.jpg', 'test_stg1/img_00161.jpg', 'test_stg1/img_07623.jpg', 'test_stg1/img_02172.jpg', 'test_stg1/img_02993.jpg', 'test_stg1/img_05979.jpg', 'test_stg1/img_06475.jpg', 'test_stg1/img_01862.jpg']\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x1b3d3ad/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now run our trained model on the test set and create a submission csv file\n",
    "\"\"\"\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "# Load the model \n",
    "model.load_weights(f'model{version}.h5')\n",
    "\n",
    "# Load the first test set, test_stg1\n",
    "# Submission must be a combination of test_stg1 (1000) and test_stg2 (12153) = 13153\n",
    "test_set1 = glob('data/test_stg1/*.jpg')\n",
    "test_set2 = glob('data/test_stg2/*.jpg')\n",
    "\n",
    "# Each image name must be called 'test_stg1/image_000001.jpg', for example\n",
    "for i, image in enumerate(test_set1):\n",
    "    test_set1[i] = 'test_stg1/' + os.path.basename(image)\n",
    "for i, image in enumerate(test_set2):\n",
    "    test_set2[i] = 'test_stg2/' + os.path.basename(image)\n",
    "test_set = test_set1 + test_set2\n",
    "print(test_set[0:10])\n",
    "\n",
    "# Create a dataframe to hold the predictions\n",
    "submission = pd.DataFrame(columns=['image', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
    "\n",
    "# For each image in the test set, run the models prediction and save it to the submission df\n",
    "# It takes about a min to run on test_stg1 for me (Karl). \n",
    "# It takes about 15 min to run the combined test_stg1 and test_stg2\n",
    "# for each iteration, it takes 20 to 40ms to run. \n",
    "for i, image in enumerate(test_set):\n",
    "    # Preprocess the test images\n",
    "    test_image = test_set[i]\n",
    "    test_image = tf.keras.preprocessing.image.load_img(test_image, target_size=(224, 224))\n",
    "    test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    test_image = keras.applications.mobilenet.preprocess_input(test_image)\n",
    "\n",
    "    # Run the prediction \n",
    "    prediction = model.predict(test_image)\n",
    "\n",
    "    # round the prediction to the nearest tenth\n",
    "    prediction = np.round(prediction, decimals=1)\n",
    "\n",
    "    # print(f'prediction array: {prediction[0]}')\n",
    "\n",
    "    # Get the predicted class\n",
    "    # The prediction array is a list of probabilities for each class\n",
    "    # For each prediction, map the prediction to the name of the class\n",
    "    # train_generator.class_indices is a dictionary of the classes of fish and their indices\n",
    "    keys = list(train_generator.class_indices.keys())\n",
    "\n",
    "    # Create a dictionary of the predictions\n",
    "    # Looks like this: \n",
    "    #  {'ALB': 1.3623509e-07, 'BET': 9.1926294e-20,\n",
    "    #   'DOL': 2.9418256e-16, 'LAG': 1.9241103e-13, 'NoF': 0.0019608391, \n",
    "    #   'OTHER': 1.6155835e-11, 'SHARK': 2.625621e-13, 'YFT': 0.99803907}\n",
    "    prediction_dict = dict(zip(keys, prediction[0]))\n",
    "    # print(prediction_dict)\n",
    "\n",
    "    # Add the prediction to the dataframe\n",
    "    submission.loc[i, 'image'] = os.path.basename(test_set[i])\n",
    "    for key in keys:\n",
    "        submission.loc[i, key] = prediction_dict[key]\n",
    "\n",
    "    # Uncomment the break if you want to test \n",
    "    # break\n",
    "\n",
    "\n",
    "# Print the head, does it look okay?\n",
    "print(submission.columns)\n",
    "print(submission.head())\n",
    "\n",
    "# Save the dataframe to a csv file :)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a495c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.974305,
   "end_time": "2023-10-12T01:05:25.764517",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-12T01:04:46.790212",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
